# End-Semester Evaluation Guide

This guide captures the artefacts, commands, and storytelling beats
needed to present the **Smart Rehearsal** project during the final
assessment.  Pair it with the coding plan and playbooks already in the
repository to demonstrate how the codebase evolved from baseline
reproduction to adaptive rehearsal and, ultimately, the modular Phase 3
pipeline.

## 1. Preparation Checklist

1. **Regenerate experiment outputs (optional)**
   - Phase 1 baseline: `python -m experiments.phase1.rehearsal_baseline --epochs 5 --buffer-size 200 --plot`
   - Phase 2 adaptive rehearsal: `python -m experiments.phase2.adaptive_rehearsal --epochs 5 --buffer-size 200 --plot`
   - Phase 3 smart pipeline: `python -m experiments.phase3.smart_rehearsal_pipeline --epochs 2 --buffer-size 300`
2. **Collect artefacts**
   - JSON/CSV metrics from `outputs/phase1` and `outputs/phase2`
   - JSONL event logs and summary JSON from `outputs/phase3`
   - Accuracy plots (`phase1_accuracy.png`, `phase2_accuracy.png`) if plotting dependencies are available
3. **Install documentation dependencies** (optional)
   - `pip install matplotlib river` if you plan to regenerate plots and use ADWIN instead of the fallback detector.

> **Tip:** All scripts support `--no-download` if datasets are already cached. Use smaller `--epochs` during dry runs to keep iteration times short when preparing slides.

## 2. Aggregating Results for the Report

The new comparison suite bundles the outputs from all phases and creates
presentation-ready summaries.

```bash
python -m experiments.end_semester.comparison_suite \
  --phase1-dir outputs/phase1 \
  --phase2-dir outputs/phase2 \
  --phase3-dir outputs/phase3 \
  --export-dir outputs/end_semester
```

This command prints a textual summary to the terminal and generates two
files under `outputs/end_semester/`:

- `comparison_report.json` â€” machine-readable snapshot for reproducibility checklists.
- `comparison_report.md` â€” Markdown table that can be dropped into the final report or slides.

If any phase directory is missing (for example, when demonstrating the
workflow without re-running training), the script will raise an explicit
error pointing to the missing artefact. This makes it easy to double
check that all experiments completed successfully before presenting to
the evaluation panel.

## 3. Storytelling Structure for the Final Presentation

1. **Recap the motivation**
   - Catastrophic forgetting, static rehearsal inefficiency, and the gap
     between fixed replay schedules and real-world drift patterns.
2. **Show the evolution across phases**
   - Phase 1: constant rehearsal baseline with exemplar buffer tracking.
   - Phase 2: ADWIN-triggered rehearsal bursts with drift event logs.
   - Phase 3: modular smart pipeline with JSONL telemetry and fallback
     detectors for low-dependency environments.
3. **Discuss comparative findings**
   - Use the aggregated report to highlight how drift-aware rehearsal
     reduces heavy replay while maintaining accuracy.
   - Emphasise compute-aware metrics (rehearsal steps/epochs) alongside
     accuracy to justify the "smart" label.
4. **Highlight engineering improvements**
   - Structured configuration dataclasses, logging infrastructure,
     plotting utilities, and reproducible runbooks.
5. **Outline next steps**
   - Extending to harder datasets (Split CIFAR-100, Tiny-ImageNet),
     experimenting with alternative detectors (DDM, Page-Hinkley), and
     integrating hyperparameter sweeps for detector calibration.

## 4. Integrating Results into the LaTeX Report

1. Copy the Markdown table generated by the comparison suite into your
   LaTeX document using the `markdown` package or convert it to a `tabular`
   environment.
2. Embed accuracy plots and drift timelines in the evaluation section.
3. Reference the JSON summary in the reproducibility appendix to show
   exact configuration and metrics captured during the run.
4. Cite the relevant literature:
   - Rebuffi et al. (2017) for iCaRL.
   - Bifet & Gavalda (2007) for ADWIN.
   - Montiel et al. (2021) for the River streaming ML library.

## 5. Troubleshooting

| Issue | Likely Cause | Quick Fix |
| --- | --- | --- |
| `FileNotFoundError` when running the comparison suite | Missing phase outputs | Re-run the specific experiment or point the script to the correct directory with `--phaseX-dir` |
| No drift events logged in Phase 2 or Phase 3 | Detector threshold too strict for the selected hyperparameters | Reduce `--detector-delta` or shorten the evaluation interval to trigger the fallback detector |
| Plots not generated | `matplotlib` not installed | Install via `pip install matplotlib` or skip the `--plot` flag |
| ADWIN import error | `river` package unavailable | Install via `pip install river` or rely on the built-in moving-average fallback |

## 6. Deliverables Recap

- âœ… Mid-semester dossier (`docs/mid-semester-review.md`)
- âœ… Phase-specific playbooks (`docs/mid-semester-runbook.md`, `docs/phase3-smart-playbook.md`)
- âœ… End-semester coding plan (`docs/end-semester-coding-plan.md`)
- âœ… End-semester evaluation guide (this file)
- ðŸ†• Comparison suite for aggregating metrics (`experiments/end_semester/comparison_suite.py`)

With these artefacts, you can walk the professor through the complete
journeyâ€”from the initial hypothesis to the adaptive rehearsal prototype
and the fully instrumented smart pipelineâ€”while backing every claim with
reproducible evidence.
