"""End-semester comparison suite for Smart Rehearsal experiments.

This utility aggregates the outputs generated by the phase 1 baseline,
phase 2 adaptive rehearsal prototype, and the phase 3 smart rehearsal
pipeline.  It loads their JSON/JSONL artefacts, computes headline
statistics (accuracy, forgetting proxy, rehearsal cost, drift events),
and emits a consolidated report that can be dropped directly into the
end-semester documentation.

The script is intentionally lightweight: it only depends on the Python
standard library so it can run in constrained evaluation environments
where heavy ML dependencies (PyTorch, torchvision, River) may not be
available.  This allows the project owner to showcase comparative
results even on machines that cannot execute the full training loops.
"""

from __future__ import annotations

import argparse
import json
from dataclasses import asdict, dataclass, field
from pathlib import Path
from statistics import mean
from typing import Dict, List, Optional


# ---------------------------------------------------------------------------
# Data containers
# ---------------------------------------------------------------------------


@dataclass
class PhaseSummary:
    """Stores headline metrics for a single experiment phase."""

    name: str
    final_accuracy: Optional[float]
    average_accuracy: Optional[float]
    best_accuracy: Optional[float]
    forgetting_gap: Optional[float]
    drift_events: int
    rehearsal_cost: float
    notes: str = ""

    def sanitized(self) -> Dict[str, object]:
        """Return a JSON-serialisable representation."""

        payload = asdict(self)
        # round floats for readability without mutating the dataclass
        for key in ("final_accuracy", "average_accuracy", "best_accuracy", "forgetting_gap", "rehearsal_cost"):
            value = payload.get(key)
            if isinstance(value, float):
                payload[key] = round(value, 4)
        return payload


@dataclass
class ComparisonReport:
    """Bundle the summaries for every phase alongside derived insights."""

    phases: List[PhaseSummary] = field(default_factory=list)

    def to_dict(self) -> Dict[str, object]:
        return {"phases": [phase.sanitized() for phase in self.phases]}

    # ------------------------------------------------------------------
    # Presentation helpers
    # ------------------------------------------------------------------

    def to_markdown(self) -> str:
        """Render a Markdown table with the consolidated metrics."""

        headers = [
            "Phase",
            "Final Acc.",
            "Avg. Acc.",
            "Best Acc.",
            "Forget Gap",
            "Drift Events",
            "Rehearsal Cost",
            "Notes",
        ]
        lines = ["| " + " | ".join(headers) + " |", "| " + " | ".join(["---"] * len(headers)) + " |"]
        for phase in self.phases:
            row = [
                phase.name,
                format_optional(phase.final_accuracy),
                format_optional(phase.average_accuracy),
                format_optional(phase.best_accuracy),
                format_optional(phase.forgetting_gap),
                str(phase.drift_events),
                format_optional(phase.rehearsal_cost),
                phase.notes or "—",
            ]
            lines.append("| " + " | ".join(row) + " |")
        return "\n".join(lines)

    def pretty_print(self) -> str:
        """Build a human-readable multi-line summary."""

        blocks = []
        for phase in self.phases:
            block = [f"Phase: {phase.name}"]
            if phase.final_accuracy is not None:
                block.append(f"  Final accuracy      : {phase.final_accuracy:.4f}")
            if phase.average_accuracy is not None:
                block.append(f"  Average accuracy    : {phase.average_accuracy:.4f}")
            if phase.best_accuracy is not None:
                block.append(f"  Best accuracy       : {phase.best_accuracy:.4f}")
            if phase.forgetting_gap is not None:
                block.append(f"  Forgetting gap      : {phase.forgetting_gap:.4f}")
            block.append(f"  Drift events        : {phase.drift_events}")
            block.append(f"  Rehearsal cost      : {phase.rehearsal_cost:.2f}")
            if phase.notes:
                block.append(f"  Notes               : {phase.notes}")
            blocks.append("\n".join(block))
        return "\n\n".join(blocks)


# ---------------------------------------------------------------------------
# File loaders
# ---------------------------------------------------------------------------


def load_json(path: Path) -> object:
    with path.open("r", encoding="utf-8") as fh:
        return json.load(fh)


def load_phase1(directory: Path) -> PhaseSummary:
    metrics_path = directory / "phase1_metrics.json"
    if not metrics_path.exists():
        raise FileNotFoundError(f"Phase 1 metrics file not found: {metrics_path}")

    records: List[Dict[str, object]] = load_json(metrics_path)  # type: ignore[assignment]
    accuracies = [float(record["accuracy"]) for record in records]
    rehearsal_sizes = [int(record["rehearsal_examples"]) for record in records]

    final_accuracy = accuracies[-1] if accuracies else None
    avg_accuracy = mean(accuracies) if accuracies else None
    best_accuracy = max(accuracies) if accuracies else None
    forgetting_gap = None
    if accuracies:
        forgetting_gap = best_accuracy - final_accuracy  # type: ignore[operator]

    notes = "Constant rehearsal baseline"
    rehearsal_cost = float(max(rehearsal_sizes) if rehearsal_sizes else 0)

    return PhaseSummary(
        name="Phase 1 Baseline",
        final_accuracy=final_accuracy,
        average_accuracy=avg_accuracy,
        best_accuracy=best_accuracy,
        forgetting_gap=forgetting_gap,
        drift_events=0,
        rehearsal_cost=rehearsal_cost,
        notes=notes,
    )


def load_phase2(directory: Path) -> PhaseSummary:
    summary_path = directory / "phase2_summary.json"
    if not summary_path.exists():
        raise FileNotFoundError(f"Phase 2 summary file not found: {summary_path}")

    payload: Dict[str, object] = load_json(summary_path)  # type: ignore[assignment]
    metrics: List[Dict[str, object]] = payload.get("metrics", [])  # type: ignore[assignment]
    events: List[Dict[str, object]] = payload.get("events", [])  # type: ignore[assignment]

    accuracies = [float(metric["accuracy"]) for metric in metrics]
    final_accuracy = accuracies[-1] if accuracies else None
    avg_accuracy = mean(accuracies) if accuracies else None
    best_accuracy = max(accuracies) if accuracies else None
    forgetting_gap = None
    if accuracies:
        forgetting_gap = best_accuracy - final_accuracy  # type: ignore[operator]

    drift_events = len(events)
    rehearsal_cost = float(sum(int(event.get("rehearsal_steps", 0)) for event in events))
    notes = "ADWIN-triggered rehearsal"

    return PhaseSummary(
        name="Phase 2 Adaptive",
        final_accuracy=final_accuracy,
        average_accuracy=avg_accuracy,
        best_accuracy=best_accuracy,
        forgetting_gap=forgetting_gap,
        drift_events=drift_events,
        rehearsal_cost=rehearsal_cost,
        notes=notes,
    )


def load_phase3(directory: Path) -> PhaseSummary:
    summary_path = directory / "summary.json"
    events_path = directory / "events.jsonl"
    if not summary_path.exists():
        raise FileNotFoundError(f"Phase 3 summary file not found: {summary_path}")
    if not events_path.exists():
        raise FileNotFoundError(f"Phase 3 events file not found: {events_path}")

    summary_payload: Dict[str, object] = load_json(summary_path)  # type: ignore[assignment]
    light_steps = float(summary_payload.get("light_rehearsal_steps", 0.0))
    heavy_epochs = float(summary_payload.get("heavy_rehearsal_epochs", 0.0))

    drift_events = 0
    eval_accuracies: List[float] = []
    with events_path.open("r", encoding="utf-8") as fh:
        for line in fh:
            line = line.strip()
            if not line:
                continue
            record = json.loads(line)
            if record.get("detector_triggered"):
                drift_events += 1
            if record.get("phase") == "eval":
                eval_accuracies.append(float(record.get("accuracy", 0.0)))

    final_accuracy = eval_accuracies[-1] if eval_accuracies else None
    avg_accuracy = mean(eval_accuracies) if eval_accuracies else None
    best_accuracy = max(eval_accuracies) if eval_accuracies else None
    forgetting_gap = None
    if eval_accuracies:
        forgetting_gap = best_accuracy - final_accuracy  # type: ignore[operator]

    rehearsal_cost = light_steps + heavy_epochs
    notes = "Smart rehearsal with modular logging"

    return PhaseSummary(
        name="Phase 3 Smart",
        final_accuracy=final_accuracy,
        average_accuracy=avg_accuracy,
        best_accuracy=best_accuracy,
        forgetting_gap=forgetting_gap,
        drift_events=drift_events,
        rehearsal_cost=rehearsal_cost,
        notes=notes,
    )


# ---------------------------------------------------------------------------
# Formatting helpers
# ---------------------------------------------------------------------------


def format_optional(value: Optional[float]) -> str:
    if value is None:
        return "—"
    return f"{value:.4f}"


# ---------------------------------------------------------------------------
# CLI
# ---------------------------------------------------------------------------


def build_arg_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description="Aggregate Smart Rehearsal experiment outputs")
    parser.add_argument("--phase1-dir", type=Path, default=Path("outputs/phase1"), help="Directory with phase 1 artefacts")
    parser.add_argument("--phase2-dir", type=Path, default=Path("outputs/phase2"), help="Directory with phase 2 artefacts")
    parser.add_argument("--phase3-dir", type=Path, default=Path("outputs/phase3"), help="Directory with phase 3 artefacts")
    parser.add_argument(
        "--export-dir",
        type=Path,
        default=Path("outputs/end_semester"),
        help="Where to store the aggregated report",
    )
    parser.add_argument(
        "--markdown-name",
        type=str,
        default="comparison_report.md",
        help="Filename for the Markdown export",
    )
    parser.add_argument(
        "--json-name",
        type=str,
        default="comparison_report.json",
        help="Filename for the JSON export",
    )
    return parser


def aggregate(args: argparse.Namespace) -> ComparisonReport:
    summaries = [
        load_phase1(args.phase1_dir),
        load_phase2(args.phase2_dir),
        load_phase3(args.phase3_dir),
    ]
    return ComparisonReport(phases=summaries)


def export_report(report: ComparisonReport, export_dir: Path, json_name: str, markdown_name: str) -> None:
    export_dir.mkdir(parents=True, exist_ok=True)

    json_path = export_dir / json_name
    with json_path.open("w", encoding="utf-8") as fh:
        json.dump(report.to_dict(), fh, indent=2)

    md_path = export_dir / markdown_name
    md_path.write_text(report.to_markdown() + "\n", encoding="utf-8")


# ---------------------------------------------------------------------------
# Entry point
# ---------------------------------------------------------------------------


def main() -> None:
    parser = build_arg_parser()
    args = parser.parse_args()

    report = aggregate(args)
    print(report.pretty_print())
    export_report(report, args.export_dir, args.json_name, args.markdown_name)
    print(f"\nSaved JSON report to {args.export_dir / args.json_name}")
    print(f"Saved Markdown report to {args.export_dir / args.markdown_name}")


if __name__ == "__main__":
    main()
